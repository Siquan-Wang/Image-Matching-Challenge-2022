{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#environment configuration：when kaggle GPU running，closeInternet\n#relative link：\n#https://www.kaggle.com/competitions/image-matching-challenge-2022\n#https://www.kaggle.com/datasets/gufanmingmie/dkm-dependecies\n#https://www.kaggle.com/datasets/ammarali32/kornia-loftr","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:29:50.027311Z","iopub.execute_input":"2022-06-06T02:29:50.027632Z","iopub.status.idle":"2022-06-06T02:29:50.055085Z","shell.execute_reply.started":"2022-06-06T02:29:50.027549Z","shell.execute_reply":"2022-06-06T02:29:50.054283Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# ***Install Libs***\n### loadkornia&kornia_moons","metadata":{}},{"cell_type":"code","source":"dry_run = False\n!pip install ../input/kornia-loftr/kornia-0.6.4-py2.py3-none-any.whl\n!pip install ../input/kornia-loftr/kornia_moons-0.1.9-py3-none-any.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-06-06T02:29:50.057333Z","iopub.execute_input":"2022-06-06T02:29:50.057815Z","iopub.status.idle":"2022-06-06T02:30:48.410229Z","shell.execute_reply.started":"2022-06-06T02:29:50.057779Z","shell.execute_reply":"2022-06-06T02:30:48.409271Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# ***Import dependencies***","metadata":{}},{"cell_type":"code","source":"#load basic operating framwork\nimport os\nimport numpy as np\nimport cv2\nimport csv\nfrom glob import glob\nimport torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport kornia\nfrom kornia_moons.feature import *\nimport kornia as K\nimport kornia.feature as KF\nimport gc\nimport sys, os, csv\nsys.path.append('../input/dkm-dependecies/DKM/')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:30:48.411720Z","iopub.execute_input":"2022-06-06T02:30:48.411994Z","iopub.status.idle":"2022-06-06T02:30:50.402549Z","shell.execute_reply.started":"2022-06-06T02:30:48.411958Z","shell.execute_reply":"2022-06-06T02:30:50.401857Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#load dkm\n!mkdir -p pretrained/checkpoints\n!cp ../input/dkm-dependecies/pretrained/dkm.pth pretrained/checkpoints/dkm_base_v11.pth\n\n!pip install -f ../input/dkm-dependecies/wheels --no-index einops\n!cp -r ../input/dkm-dependecies/DKM/ /kaggle/working/DKM/\n!cd /kaggle/working/DKM/; pip install -f ../input/dkm-dependecies/wheels -e . ","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:30:50.404444Z","iopub.execute_input":"2022-06-06T02:30:50.404710Z","iopub.status.idle":"2022-06-06T02:31:35.811286Z","shell.execute_reply.started":"2022-06-06T02:30:50.404677Z","shell.execute_reply":"2022-06-06T02:31:35.810434Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# ***Model***","metadata":{}},{"cell_type":"code","source":"#loftr load\ndevice = torch.device('cuda')#use GPU\nmatcher = KF.LoFTR(pretrained=None)# load LoFTR structure\nmatcher.load_state_dict(torch.load(\"../input/kornia-loftr/loftr_outdoor.ckpt\")['state_dict'])#load weight\nmatcher = matcher.to(device).eval()#trasnfer to GPU\n\nIMG_MAX_SIZE = [1280, 840]#use two sacles to TTA（Test-time augment）infer\nDO_FLIP = True #decide image flip\nMAX_NUM_PAIRS = 8000 #max matching points\n\n##load dkm\ntorch.hub.set_dir('/kaggle/working/pretrained/')\nfrom dkm import dkm_base\ndkm_model = dkm_base(pretrained=True, version=\"v11\").to(device).eval()","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:31:35.812883Z","iopub.execute_input":"2022-06-06T02:31:35.813184Z","iopub.status.idle":"2022-06-06T02:31:41.327332Z","shell.execute_reply.started":"2022-06-06T02:31:35.813148Z","shell.execute_reply":"2022-06-06T02:31:41.326368Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## *Utils*","metadata":{}},{"cell_type":"code","source":"src = '/kaggle/input/image-matching-challenge-2022/'\n\n#load test info\ntest_samples = []\nwith open(f'{src}/test.csv') as f:\n    reader = csv.reader(f, delimiter=',')\n    for i, row in enumerate(reader):\n        # Skip header.\n        if i == 0:\n            continue\n        test_samples += [row]\n\n#flattern matrix\ndef FlattenMatrix(M, num_digits=8):\n    '''Convenience function to write CSV files.'''\n    \n    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n\n\n#loftr\ndef load_torch_image(fname, device, resize=True, img_max_size=1280):\n    img = cv2.imread(fname)\n    img_sz = img.shape\n    if img_max_size <=0:\n        resize = False\n    #change shape\n    if resize:\n        #use leargets scalescale\n        scale = img_max_size / max(img.shape[0], img.shape[1]) \n    else:\n        scale = 1\n    w = int(img.shape[1] * scale)\n    h = int(img.shape[0] * scale)\n    #normalization，channel\n    img = cv2.resize(img, ((w//8)*8, (h//8)*8))\n    img = K.image_to_tensor(img, False).float() /255.\n    img = K.color.bgr_to_rgb(img)\n    return img.to(device), img_sz\n\n#single match\ndef match_same_size(img_path0, img_path1, matcher, device=device, flip=False, img_max_size=1280):\n    img0, source_img_shape = load_torch_image(img_path0, device, img_max_size=img_max_size)\n    img1, target_img_shape = load_torch_image(img_path1, device, img_max_size=img_max_size)\n    input_dict = {\"image0\": K.color.rgb_to_grayscale(img0).to(device), \n                  \"image1\": K.color.rgb_to_grayscale(img1).to(device)}\n    #\n    if flip:\n        input_dict[\"image0\"] = torch.cat((input_dict[\"image0\"], torch.flip(input_dict[\"image0\"], dims=[3])), dim=0)\n        input_dict[\"image1\"] = torch.cat((input_dict[\"image1\"], torch.flip(input_dict[\"image1\"], dims=[3])), dim=0)\n        \n    #infer    \n    with torch.no_grad():\n        correspondences = matcher(input_dict)\n    \n    # flip coordinate\n    #extract important info\n    # https://github.com/kornia/kornia/blob/master/kornia/feature/loftr/loftr.py\n    keypoints0 = correspondences[\"keypoints0\"].cpu().numpy()\n    keypoints1 = correspondences[\"keypoints1\"].cpu().numpy()\n    confidence = correspondences[\"confidence\"].cpu().numpy()\n    batch_indexes = correspondences[\"batch_indexes\"].cpu().numpy()\n    if flip:\n        select_fliped = (batch_indexes == 1)\n        keypoints0[select_fliped, 0] = input_dict[\"image0\"].shape[-1] - keypoints0[select_fliped, 0]\n        keypoints1[select_fliped, 0] = input_dict[\"image1\"].shape[-1] - keypoints1[select_fliped, 0]\n    \n    # scale\n    #chaneg back to origin\n    new_source_img_shape = input_dict[\"image0\"].shape[-2:]\n    new_target_img_shape = input_dict[\"image1\"].shape[-2:]\n    sy, sx = source_img_shape[0] / new_source_img_shape[0], source_img_shape[1] / new_source_img_shape[1]\n    keypoints0[:, 0] = keypoints0[:, 0] * sx\n    keypoints0[:, 1] = keypoints0[:, 1] * sy\n\n    sy, sx = target_img_shape[0] / new_target_img_shape[0], target_img_shape[1] / new_target_img_shape[1]\n    keypoints1[:, 0] = keypoints1[:, 0] * sx\n    keypoints1[:, 1] = keypoints1[:, 1] * sy\n    torch.cuda.empty_cache()\n    return {\n        \"keypoints0\": keypoints0,\n        \"keypoints1\":  keypoints1,\n        \"confidence\": confidence\n    }\n    \n#put single pair into multiple pairs   \ndef match(img_path0, img_path1, matcher, device=device):\n    mkpts0 = []\n    mkpts1 = []\n    confidence = []\n    for img_size in IMG_MAX_SIZE:\n        preds =  match_same_size(img_path0, img_path1, matcher, device=device, flip=DO_FLIP, img_max_size=img_size)\n        mkpts0.append(preds[\"keypoints0\"])\n        mkpts1.append(preds[\"keypoints1\"])\n        confidence.append(preds[\"confidence\"])\n    # concatenate\n    mkpts0 = np.concatenate(mkpts0, axis=0)\n    mkpts1 = np.concatenate(mkpts1, axis=0)\n    confidence = np.concatenate(confidence, axis=0)\n    \n    # filter pairs\n    #filterMAX_NUM_PAIRS\n    ind = np.argsort(-confidence)\n    ind = ind[:MAX_NUM_PAIRS]\n    mkpts0 = mkpts0[ind]\n    mkpts1 = mkpts1[ind]\n    confidence = confidence[ind]\n    return mkpts0, mkpts1","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:31:41.328879Z","iopub.execute_input":"2022-06-06T02:31:41.329134Z","iopub.status.idle":"2022-06-06T02:31:41.358186Z","shell.execute_reply.started":"2022-06-06T02:31:41.329101Z","shell.execute_reply":"2022-06-06T02:31:41.357463Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# ***Inference***","metadata":{}},{"cell_type":"code","source":"F_dict = {}\nimport time\nfor i, row in enumerate(test_samples):\n    #get info\n    sample_id, batch_id, image_1_id, image_2_id = row\n    \n    #loftr model extrac keypoint-pairs \n    st = time.time()\n    img_path0 = f'{src}/test_images/{batch_id}/{image_1_id}.png'\n    img_path1 = f'{src}/test_images/{batch_id}/{image_2_id}.png'\n    mkpts0, mkpts1 = match(img_path0, img_path1, matcher)\n    torch.cuda.empty_cache()\n    \n    ##dkm\n    img1 = cv2.imread(img_path0) \n    img2 = cv2.imread(img_path1)\n        \n    img1PIL = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n    img2PIL = Image.fromarray(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n    \n    #dkm extrac keypoint-pairs \n    dense_matches, dense_certainty = dkm_model.match(img1PIL, img2PIL)\n    dense_certainty = dense_certainty.sqrt()\n    sparse_matches, sparse_certainty = dkm_model.sample(dense_matches, dense_certainty, 2000)\n    #filter low score point\n    sparse_matches = sparse_matches[sparse_certainty>0.5]\n    dkm_p0 = sparse_matches[:, :2]\n    dkm_p1 = sparse_matches[:, 2:]\n    \n    #rescale \n    h, w, c = img1.shape\n    dkm_p0[:, 0] = ((dkm_p0[:, 0] + 1)/2) * w\n    dkm_p0[:, 1] = ((dkm_p0[:, 1] + 1)/2) * h\n\n    h, w, c = img2.shape\n    dkm_p1[:, 0] = ((dkm_p1[:, 0] + 1)/2) * w\n    dkm_p1[:, 1] = ((dkm_p1[:, 1] + 1)/2) * h\n    \n    #ensemble\n    mkpts0 = np.concatenate([mkpts0, dkm_p0], axis=0)\n    mkpts1 = np.concatenate([mkpts1, dkm_p1], axis=0)\n    #最少需要8个点来得到基础矩阵（使用8点算法）\n    if len(mkpts0) > 7:\n        #使用cv2.findFundamentalMat计算基础矩阵，使用cv2.USAC_MAGSAC估算器，匹配误差在0.1845-0.999999，最大迭代次数为10000\n        F, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.1845, 0.999999, 10000)\n        inliers = inliers > 0\n        assert F.shape == (3, 3), 'Malformed F?'\n        F_dict[sample_id] = F\n    else:\n        F_dict[sample_id] = np.zeros((3, 3))\n        continue\n    gc.collect()\n    nd = time.time()    \n    #画图相关函数\n    if (i < 3):\n        print(\"Running time: \", nd - st, \" s\")\n        image_1 =load_torch_image(img_path0, device=device, resize=False)[0]\n        image_2 = load_torch_image(img_path1, device=device,  resize=False)[0]\n        draw_LAF_matches(\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts0).view(1,-1, 2),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts0.shape[0]).view(1,-1, 1)),\n\n        KF.laf_from_center_scale_ori(torch.from_numpy(mkpts1).view(1,-1, 2),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1, 1),\n                                    torch.ones(mkpts1.shape[0]).view(1,-1, 1)),\n        torch.arange(mkpts0.shape[0]).view(-1,1).repeat(1,2),\n        K.tensor_to_image(image_1),\n        K.tensor_to_image(image_2),\n        inliers,\n        draw_dict={'inlier_color': (0.2, 1, 0.2),\n                   'tentative_color': None, \n                   'feature_color': (0.2, 0.5, 1), 'vertical': False})\n    \n#结果文件保存\nwith open('submission.csv', 'w') as f:\n    f.write('sample_id,fundamental_matrix\\n')\n    for sample_id, F in F_dict.items():\n        f.write(f'{sample_id},{FlattenMatrix(F)}\\n')","metadata":{"execution":{"iopub.status.busy":"2022-06-06T02:31:41.359607Z","iopub.execute_input":"2022-06-06T02:31:41.360113Z","iopub.status.idle":"2022-06-06T02:32:42.590286Z","shell.execute_reply.started":"2022-06-06T02:31:41.360058Z","shell.execute_reply":"2022-06-06T02:32:42.589361Z"},"trusted": true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
